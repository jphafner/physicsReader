---
layout: ebook
title: The Law of Disorder
author: George Gamow
exerpt: Completely random motion, such as the thermal motion of molecules, might seem to be out of the realm of lawfulness.
    But on the contrary. Just because the motion is completely disorderly, it is subject to statistical laws.
intro: A chapter from his book <em>One, Two, Three &hellip; Infinity</em>, 1947.
---

<span class="textsc">If you</span> pour a glass of water and look at it, you will see a clear uniform fluid with no trace of any internal structure or motion in it whatsoever (provided, of course, you do not shake the glass).
We know, however, that the uniformity of water is only apparent and that if the water is magnified a few million times, there will be revealed a strongly expressed granular structure formed by a large number of separate molecules closely packed together.

Under the same magnification it is also apparent that the water is far from still, and that its molecules are in a state of violent agitation moving around and pushing one another as though they were people in a highly excited crowd.
This irregular motion of water molecules, or the molecules of any other material substance, is known as heat (or thermal) motion, for the simple reason that it is responsible for the phenomenon of heat.
For, although molecular motion as well as molecules themselves are not directly discernible to the human eye, it is molecular motion that produces a certain irritation in the nervous fibers of the human organism and produces the sensation that we call heat.
For those organisms that are much smaller than human beings, such as, for example, small bacteria suspended in a water drop, the effect of thermal motion is much more pronounced, and these poor creatures are incessantly kicked, pushed, and tossed around by the restless molecules that attack them from all sides and give them no rest (Figure 77).
This amusing phenomenon, known as Brownian motion, named after the English botanist Robert Brown, who first noticed it more than a century ago in a study of tiny plant spores, is of quite general nature and can be observed in the study of any kind of sufficiently small particles suspended in any kind of liquid, or of microscopic particles of smoke and dust floating in the air.



If we heat the liquid the wild dance of tiny particles suspended in it becomes more violent; with cooling the intensity of the motion noticeably subsides.
This leaves no doubt that we are actually watching here the effect of the hidden thermal motion of matter, and that what we usually call temperature is nothing else but a measurement of the degree of molecular agitation.
By studying the dependence of Brownian motion on temperature, it was found that at the temperature of &minus;273&nbsp;&deg;C or &minus;459&nbsp;&deg;F, thermal agitation of matter completely ceases, and all its molecules come to rest.
This apparently is the lowest temperature and it has received the name of absolute zero.
It would be an absurdity to speak about still lower temperatures since apparently there is no motion slower than absolute rest!

<figure id="fig77">
<figcaption>
    Six consecutive positions of a bacterium which is being tossed around by molecular impacts (physically correct; bacteriologically not quite so).
</figcaption>
</figure>

Near the absolute zero temperature the molecules of any substance have so little energy that the cohesive forces acting upon them cement them together into one solid block, and all they can do is only quiver slightly in their frozen state.
When the temperature rises the quivering becomes more and more intense, and at a certain stage our molecules obtain some freedom of motion and are able to slide by one another.
The rigidity of the frozen substance disappears, and it becomes a fluid.
The temperature at which the melting process takes place depends on the strength of the cohesive forces acting upon the molecules.
In some materials such as hydrogen, or a mixture of nitrogen and oxygen which form atmospheric air, the cohesion of molecules is very weak, and the thermal agitation breaks up the frozen state at comparatively low temperatures.
Thus hydrogen exists in the frozen state only at temperatures below 14&deg; abs (i.e,, below &minus;259&nbsp;&deg;C, whereas solid oxygen and nitrogen melt at 55&deg; abs and 64&deg; abs, respectively (i.e. &minus;218&nbsp;&deg;C and &minus;209&nbsp;&deg;C).
In other substances the cohesion between molecules is stronger and they remain solid up to higher temperatures: thus pure alcohol remains frozen up to &minus;130 &deg;C, whereas frozen water (ice) melts only at 0&nbsp;&deg;C.
Other substances remain solid up to much higher temperatures; a piece of lead will melt only at +327&nbsp;&deg;C, iron at +1535&nbsp;&deg;C, and the rare metal known as osmium remains solid up to the temperature of +2700&nbsp;&deg;C.
Although in the solid state of matter the molecules are strongly bound to their places, it does not mean at all that they are not affected by thermal agitation.
Indeed, according to the fundamental law of heat motion, the amount of energy in every molecule is the same for all substances, solid, liquid, or gaseous at a given temperature, and the difference lies only in the fact that whereas in some cases this energy suffices to tear off the molecules from their fixed positions and let them travel around, in other cases they can only quiver on the same spot as angry dogs restricted by short chains.

This thermal quivering or vibration of molecules forming a solid body can be easily observed in the X-ray photographs described in the previous chapter.
We have seen indeed that, since taking a picture of molecules in a crystal lattice requires a considerable time, it is essential that they should not move away from their fixed positions during the exposure.
But a constant quivering around the fixed position is not conducive to good photography, and results in a somewhat blurred picture.
This effect is shown in the molecular photograph which is reproduced in Plate I.
To obtain sharper pictures one must cool the crystals as much as possible.
This is sometimes accomplished by dipping them in liquid air.
If, on the other hand, one warms up the crystal to be photographed, the picture becomes more and more blurred, and, at the melting point the pattern completely vanishes, owing to the fact that the molecules leave their places and begin to move in an irregular way through the melted substance.

<figure>
    \label{fig:78}
</figure>


After solid material melts, the molecules still remain together, since the thermal agitation, though strong enough to dislocate them from the fixed position in the crystalline lattice, is not yet sufficient to take them completely apart.
At still higher temperatures, however, the cohesive forces are not able to hold the molecules together any more and they fly apart in all directions unless prevented from doing so by the surrounding walls.
When this happens, of course, the result is matter in a gaseous state.
As in the melting of a solid, the evaporation of liquids takes place at different temperatures for different materials, and the substances with a weaker internal cohesion will turn into vapor at lower temperatures than those in which cohesive forces are stronger.
In this case the process also depends rather essentially on the pressure under which the liquid is kept, since the outside pressure evidently helps the cohesive forces to keep the molecules together.
Thus, as everybody knows, water in a tightly closed kettle boils at a lower temperature than will water in an open one.
On the other hand, on the top of high mountains, where atmospheric pressure is considerably less, water will boil well below 100&nbsp;&deg;C.
It may be mentioned here that by measuring the temperature at which water will boil, one can calculate atmospheric pressure and consequently the distance above sea level of a given location.


But do not follow the example of Mark Twain who, according to his story, once decided to put an aneroid barometer into a boiling kettle of pea soup.
This will not give you any idea of the elevation, and the copper oxide will make the soup taste bad.

The higher the melting point of a substance, the higher is its boiling point.
Thus liquid hydrogen boils at &minus;253&nbsp;&deg;C, liquid oxygen and nitrogen at &minus;183&nbsp;&deg;C and &minux;196&nbsp;&deg;C,
alcohol at +78&nbsp;&deg;C, lead at +1620&nbsp;&deg;C, iron at +3000&nbsp;&deg;C and osmium only above +5300&nbsp;&deg;C. [^1]


The breaking up of the beautiful crystalline structure of solid bodies forces the molecules first to crawl around one another like a pack of worms, and then to fly apart as though they were a flock of frightened birds.
But this latter phenomenon still does not represent the limit of the destructive power of increasing thermal motion.
If the temperature rises still farther the very existence of the molecules is threatened, since the ever increasing violence of intermolecular collisions is capable of breaking them up into separate atoms.
This thermal dissociation, as it is called, depends on the relative strength of the molecules subjected to it.
The molecules of some organic substances will break up into separate atoms or atomic groups at temperatures as low as a few hundred degrees.
Other more sturdily built molecules, such as those of water, will require a temperature of over a thousand degrees to be destroyed.
But when the temperature rises to several thousand degrees no molecules will be left and the matter will be a gaseous mixture of pure chemical elements.


This is the situation on the surface of our sun where the temperature ranges up to 6000&nbsp;&deg;C.
On the other hand, in the comparatively cooler atmospheres of the red stars, [^2] some of the molecules are still present, a fact that has been demonstrated by the methods of spectral analysis.

The violence of thermal collisions at high temperatures not only breaks up the molecules into their constituent atoms, but also damages the atoms themselves by chipping off their outer electrons.
This thermal ionization becomes more and more pronounced when the temperature rises into tens and hundreds of thousands of degrees, and reaches completion at a few million degrees above zero.
At these tremendously hot temperatures, which are high above everything that we can produce in our laboratories but which are common in the interiors of stars and in particular inside our sun, the atoms as such cease to exist.
All electronic shells are completely stripped off, and the matter becomes a mixture of bare nuclei and free electrons rushing wildly through space and colliding with one another with tremendous force.
However, in spite of the complete wreckage of atomic bodies, the matter still retains its fundamental chemical characteristics, inasmuch as atomic nuclei remain intact.
If the temperature drops, the nuclei will recapture their electrons and the integrity of atoms will be reestablished.

<figure>
<figcaption>
    Photograph of Hexamethylbenzene molecule magnified 175,000,000 times.
    Courtesy of Dr. M. L. Huggins. <em>Eastman Kodak Laboratory</em>.
</figcaption>
</figure>

<figure id="fig79">
<figcaption>
    The destructive effect of temperature.
</figcaption>
</figure>


In order to attain complete thermal dissociation of matter, that is to break up the nuclei themselves into the separate nucleons (protons and neutrons) the temperature must go up to at least several billion degrees.
Even inside the hottest stars we do not find such high temperatures, though it seems very likely that temperatures of that magnitude did exist several billion years ago when our universe was still young.
We shall return to this exciting question in the last chapter of this book.


Thus we see that the effect of thermal agitation is to destroy step by step the elaborate architecture of matter based on the law of quantum, and to turn this magnificent building into a mess of widely moving particles rushing around and colliding with one another without any apparent law or regularity.


## How Can One Describe Disorderly Motion?


It would be, however, a grave mistake to think that because of the irregularity of thermal motion it must remain outside the scope of any possible physical description.
Indeed the fact itself that thermal motion is completely irregular makes it subject to a new kind of law, the Law of Disorder better known as the Law of Statistical Behavior.
In order to understand the above statement let us turn our attention to the famous problem of a &ldquo;Drunkard's Walk.&rdquo; Suppose we watch a drunkard who has been leaning against a lamp post in the middle of a large paved city square (nobody knows how or when he got there) and then has suddenly decided to go nowhere in particular.
Thus off he goes, making a few steps in one direction, then some more steps in another, and so on and so on, changing his course every few steps in an entirely unpredictable way (Figure \ref{fig:80}).
How far will be our drunkard from the lamp post after he has executed, say, a hundred phases of his irregular zigzag journey?
One would at first think that, because of the unpredictability of each turn, there is no way of answering this question.
If, however, we consider the problem a little more attentively we will find that, although we really cannot tell where the drunkard will be at the end of his walk, we can answer the question about his most probable distance from the lamp post after a given large number of turns.
In order to approach this problem in a vigorous mathematical way let us draw on the pavement two co-ordinate axes with the origin in the lamp post; the X-axis coming toward us and the Y-axis to the right.
Let <span class="math">R</span> be the distance of the drunkard from the lamp post after the total of <span class="math">N</span> zigzags (14 in Figure \ref{fig:80}).
If now <span class="math">X_N</span> and <span class="math">Y_N</span> are the projections of the N<sup>th</sup> leg of the track on the corresponding axis, the Pythagorean theorem gives us apparently:

\begin{equation}
    R^2 = \left( X_1 + X_2 + X_3 + \cdots + X_N \right)^2 + \left( Y_1 + Y_2 + Y_3 + \cdots + Y_N \right)^2
\end{equation}

where <span class="math">X</span>'s and <span class="math">Y</span>'s are positive or negative depending on whether our drunkard was moving to or from the post in this particular phase of his walk.
Notice that since his motion is completely disorderly, there will be about as many positive values of <span class="math">X</span>'s and <span class="math">Y</span>'s as there are negative.
In calculating the value of the square of the terms in parentheses according to the elementary rules of algebra, we have to multiply each term in the bracket by itself and by each of all other terms.

<figure>
\label{fig:80}
\caption{Drunkard's walk.}
</figure>


Thus:

\begin{align}
    & \left( X_1 + X_2 + X_3 + \cdots + X_N \right)^2 \\
    &= \left( X_1 + X_2 + X_3 + \cdots + X_N \right)^2
        \left( X_1 + X_2 + X_3 + \cdots + X_N \right)^2 \\
    &= X_1^2 + X_1 X_2 + X_1 X_3 + \cdots + X_2^2 + X_1 X_2 + \cdots + X_N^2
\end{align}

This long sum will contain the square of all <span class="math">X</span>'s $( X_1^2, X_2^2, \cdots\ , X_N^2)$, and the so-called &ldquo;mixed products&rdquo; like <span class="math">X<sub>1</sub>X<sub>2</sub></span>, <span class="math">X<sub>2</sub>X<sub>3</sub></span>, etc.


So far it is simple arithmetic, but now comes the statistical point based on the disorderliness of the drunkard's walk.
Since he was moving entirely at random and would just as likely make a step toward the post as away from it, the values of X's have a fifty-fifty chance of being either positive or negative.
Consequently in looking through the &ldquo;mixed products&rdquo; you are likely to find always the pairs that have the same numerical value but opposite signs thus canceling each other, and the larger the total number of turns, the more likely it is that such a compensation takes place.
What will be left are only the squares of X's, since the square is always positive.
Thus the whole thing can be written as <span class="math">X<sub>1</sub><sup>2</sup> + X<sub>2</sub><sup>2</sup> + &middot; + X<sub>N</sub><sup>2</sup> = N X<sup>2</sup></span> where <span class="math">X</span> is the average length of the projection of a zigzag link on the X-axis.


In the same way we find that the second bracket containing <span class="math">Y</span>'s can be reduced to:
    NY<sup>2</sup>, <span class="math">Y</span> being the average projection of the link on the <span class="math">Y</span>-axis.
It must be again repeated here that what we have just done is not strictly an algebraic operation, but is based on the statistical argument concerning the mutual cancellation of &ldquo;mixed products&rdquo; because of the random nature of the pass.
For the most probable distance of our drunkard from the lamp post we get now simply:

\begin{equation}
    R_2 = N (X^@ + Y^2)
\end{equation}
or
\begin{equation}
    R = \sqrt{N} \cdot \sqrt{X^2 + Y^2}
\end{equation}

But the average projections of the link on both axes is simply a 45&deg; projection, so that $\sqrt{ X^2 + Y^2}$ right is (again because of the Pythagorean theorem) simply equal to the average length of the link.
Denoting it by 1 we get:

\begin{equation}
R = l'y/N
\end{equation}

In plain words our result means:
    <em>the most probable distance of out drunkard from the lamp post after a certain large number of irregular turns is equal to the average length of each straight track that he walks, times the square root of their number.</em>


Thus if our drunkard goes one yard each time before he turns (at an unpredictable angle!), he will most probably be only ten yards from the lamp post after walking a grand total of a hundred yards.
If he had not turned, but had gone straight, he would be a hundred yards away&mdash;which shows that it is definitely advantageous to be sober when taking a walk.

<figure id="fig81">
<figcaption>
    Statistical distribution of six walking drunkards around the lamp post.
</figcaption>
</figure>


The statistical nature of the above example is revealed by the fact that we refer here only to the most probable distance and not to the exact distance in each individual case.
In the case of an individual drunkard it may happen, though this is not very probable, that he does not make any turns at all and thus goes far away from the lamp post along the straight line.
It may also happen, that he turns each time by, say, 180 degrees thus returning to the lamp post after every second turn.
But if a large number of drunkards all start from the same lamp post walking in different zigzag paths and not interfering with one another you will find after a sufficiently long time that they are spread over a certain area around the lamp post in such a way that their average distance from the post may be calculated by the above rule.
An example of such spreading due to irregular motion is given in Figure \ref{fig:81}, where we consider six walking drunkards.
It goes without saying that the larger the number of drunkards, and the larger the number of turns they make in their disorderly walk, the more accurate is the rule.


Now substitute for the drunkards some microscopic bodies such as plant spores or bacteria suspended in liquid, and you will have exactly the picture that the botanist Brown saw in his microscope.
True the spores and bacteria are not drunk, but, as we have said above, they are being incessantly kicked in all possible directions by the surrounding molecules involved in thermal motion, and are therefore forced to follow exactly the same irregular zigzag trajectories as a person who has completely lost his sense of direction under the influence of alcohol.


If you look through a microscope at the Brownian motion of a large number of small particles suspended in a drop of water, you will concentrate your attention on a certain group of them that are at the moment concentrated in a given small region (near the &ldquo;lamp post&rdquo;).
You will notice that in the course of time they become gradually dispersed all over the field of vision, and that their average distance from the origin increases in proportion to the square root of the time interval as required by the mathematical law by which we calculated the distance of the drunkard's walk.


The same law of motion pertains, of course, to each separate molecule in our drop of water; but you cannot see separate molecules, and even if you could, you wouldn't be able to distinguish between them.
To make such motion visible one must use two different kinds of molecules distinguishable for example by their different colors.
Thus we can fill one half of a chemical test tube with a water solution of potassium permanganate, which will give to the water a beautiful purple tint.
If we now pour on the top of it some clear fresh water, being careful not to mix up the two layers, we shall notice that the color gradually penetrates the clear water.
If you wait sufficiently long you will find that all the water from the bottom to the surface becomes uniformly colored.
This phenomenon, familiar to everybody, is known as diffusion and is due to the irregular thermal motion of the molecules of dye among the water molecules.
We must imagine each molecule of potassium permanganate as a little drunkard who is driven to and fro by the incessant impacts received from other molecules.
Since in water the molecules are packed rather tightly (in contrast to the arrangement of those in a gas) the average free path of each molecule between two successive collisions is very short, being only about one hundred millionths of an inch.
Since on the other hand the molecules at room temperature move with the speed of about one tenth of a mile per second, it takes only one million-millionth part of a second for a molecule to go from one collision to another.
Thus in the course of a single second each dye molecule will be engaged in about a million million consecutive collisions and will change its direction of motion as many times.
The average distance covered during the first second will be one hundred millionth of an inch (the length of free path) times the square root of a million millions.
This gives the average diffusion speed of only one hundredth of an inch per second;
    a rather slow progress considering that if it were not deflected by collisions, the same molecule would be a tenth of a mile away!
If you wait 100 seconds, the molecule will have struggled through 10 times ($\sqrt{100} = 10$) as great distance, and in 10,000 sec, that is, in about 3 hours, the diffusion will have carried the coloring 100 times farther ($\sqrt{10000} = 100$), that is, about 1 inch away.
Yes, diffusion is a rather slow process; when you put a lump of sugar into your cup of tea you had better stir it rather than wait until the sugar molecules have been spread throughout by their own motion.


<figure id="fig82">
</figure>


Just to give another example of the process of diffusion, which is one of the most important processes in molecular physics, let us consider the way in which heat is propagated through an iron poker, one end of which you put into the fireplace.
From your own experience you know that it takes quite a long time until the other end of the poker becomes uncomfortably hot, but you probably do not know that the heat is carried along the metal stick by the process of diffusion of electrons.
Yes, an ordinary iron poker is actually stuffed with electrons, and so is any metallic object.
The difference between a metal, and other materials, as for example glass, is that the atoms of the former lose some of their outer electrons, which roam all through the metallic lattice, being involved in irregular thermal motion, in very much the same way as the particles of ordinary gas.


The surface forces on the outer boundaries of a piece of metal prevent these electrons from getting out, [^3] but in their motion inside the material they are almost perfectly free.
If an electric force is applied to a metal wire, the free unattached electrons will rush headlong in the direction of the force producing the phenomenon of electric current.
The nonmetals on the other hand are usually good insulators because all their electrons are bound to be atoms and thus cannot move freely.


When one end of a metal bar is placed in the fire, the thermal motion of free electrons in this part of the metal is considerably increased, and the fast-moving electrons begin to diffuse into the other regions carrying with them the extra energy of heat.
The process is quite similar to the diffusion of dye molecules through water, except that instead of having two different kinds of particles (water molecules and dye molecules) we have here the diffusion of hot electron gas into the region occupied by cold electron gas.
The drunkard's walk law applies here, however, just as well and the distances through which the heat propagates along a metal bar increase as the square roots of corresponding times.


As our last example of diffusion we shall take an entirely different case of cosmic importance. As we shall learn in the fol- lowing chapters the energy of our sun is produced deep in its interior by the alchemic transformation of chemical elements.
This energy is liberated in the form of intensive radiation, and the &ldquo;particles of light,&rdquo; or the light quanta begin their long journey through the body of the sun towards its surface.
Since light moves at a speed of 300,000 km/sec, and the radius of the sun is only 700,000 km it would take a light quantum only slightly over two seconds to come out provided it moved without any deviations from a straight line.
However, this is far from being the case; on their way out the light quanta undergo innumerable collisions with the atoms and electrons in the material of the sun.
The free pass of a light quantum in solar matter is about a centimeter (much longer than a free pass of a molecule!) and since the radius of the sun is 70,000,000,000 centimeter, our light quantum must make $\left(7\times 10^{10}\right)^2$ or $5 \times 10^{21}$ drunkard's steps to reach the surface.

Since each step requires $\frac{1}{3\times 10^{10}}$ $3\times 10^9$ second, the entire time of travel is $3\times 10^{-9}\,\text{sec}\times 5\times 10^{21} = 1.5\times 10^{13}\,\text{sec}$ or about 200,000 years!
Here again we see how slow the process of diffusion is.
It takes light 2,000 centuries to travel from the center of the sun to its surface, whereas after coming into empty intraplanetary space and traveling along a straight line it covers the entire distance from the sun to the earth in only eight minutes!

## Counting Probabilities

This case of diffusion represents only one simple example of the application of the statistical law of probability to the problem of molecular motion.
Before we go farther with that discussion, and make the attempt to understand the all-important Law of Entropy, which rules the thermal behavior of every material body, be it a tiny droplet of some liquid or the giant universe of stars, we have first to learn more about the ways in which the probability of different simple or complicated events can be calculated.


By far the simplest problem of probability calculus arises when you toss a coin.
Everybody knows that in this case (without cheating) there are equal chances to get heads or tails.
One usually says that there is a fifty-fifty chance for heads or tails, but it is more customary in mathematics to say that the chances are half and half.
If you add the chances of getting heads and getting tails you get $\frac{1}{2} + \frac{1}{2} = 1$.
Unity in the theory of probability means a certainty; you are in fact quite certain that in tossing a coin you get either heads or tails, unless it rolls under the sofa and vanishes tracelessly.


<figure id="fig83">
<figcaption>
Four possible combinations in tossing two coins.
</figcaption>
</figure>


Suppose now you drop the coin twice in succession or, what is the same, you drop 2 coins simultaneously.
It is easy to see that you have here 4 different possibilities shown in Figure \ref{fig:83}.


In the first case you get heads twice, in the last case tails twice, whereas the two intermediate cases lead to the same result since it does not matter to you in which order (or in which coin) heads or tails appear.
Thus you say that the chances of getting heads twice are 1 out of 4 or <span class="math">1/4</span> the chances of getting tails twice are also <span class="math">1/4</span>,
    whereas the chances of heads once and tails once are 2 out of 4 or <span class="math">1/2</span>.
Here again <span class="math">1/4 + 1/4 + 1/2  = 1<span> meaning that you are certain to get one of the 3 possible combinations. Let us see now what happens if we toss the coin 3 times.
There are altogether 8 possibilities summarized in the following table:

\begin{table}
\begin{tabu}{lccccccccc}
    First tossing   & h & h  & h & h  & t & t  & t & t  \\
    Second          & h & h  & t & t  & h & h  & t & t  \\
    Third           & h & t  & h & t  & h & t  & h & t  \\
                    & I & II & III & II & III & III & IV \\
\end{tabu}
\end{table}


If you inspect this table you find that there is 1 chance out of 8 of getting heads three times, and the same of getting tails three times.
The remaining possibilities are equally divided between heads twice and tails once, or heads once and tails twice, with the probability three eighths for each event.


Our table of different possibilities is growing rather rapidly, but let us take one more step by tossing 4 times. Now we have the following 16 possibilities:

\begin{table}
\begin{tabu}{lcccccccccccccccc}
    First tossing   & h & h  & h & h  & h & h  & h & h & t & t & t & t  & t & t & t & t  \\
    Second          & h & h  & h & h  & t & t  & t & t & h & h & h & h  & t & t & t & t  \\
    Third           & h & h  & t & t  & h & h  & h & h & t & t & h & h  & t & t & h & h  \\
    Fourth          & h & t  & h & t  & h & t  & h & t & h & t & h & t  & h & t & h & t  \\
    & I & II & II & III & II & III & III & IV & II & III & III & IV & III & IV & IV & V \\
\end{tabu}
\end{table}


Here we have <span class="math">1/16</span> for the probability of heads four times, and exactly the same for tails four times.
The mixed cases of heads three times and tails once or tails three times and heads once have the probabilities of <span class="math">4/16</span> or <span class="math">1/4</span> each, whereas the chances of heads and tails the same number of times are <span class="math">6/16</span> or <span class="math">3/8</span>.
If you try to continue in a similar way for larger numbers of tosses the table becomes so long that you will soon run out of paper; thus for example for ten tosses you have 1024 different possibilities (i.e., <span class="math">2&times; 2&times; 2\&times; 2&times; 2&times; 2&times; 2&times; 2&times; 2&times; 2</span>).
But it is not at all necessary to construct such long tables since the simple laws of probability can be observed in those simple examples that we already have cited and then used directly in more complicated cases.

First of all you see that the probability of getting heads twice is equal to the product of the probabilities of getting it separately in the first and in the second tossing; in fact <span class="math">1/4 = 1/2 &times; 1/2</span>.
Similarly the probability of getting heads three or four times in succession is the product of probabilities of getting it separately in each tossing ($\frac{1}{8} = \frac{1}{2} \times \frac{1}{2} \times \frac{1}{2}; \frac{1}{16} = \frac{1}{2} \times \frac{1}{2} \times \frac{1}{2} \times \frac{1}{2}$.
Thus if somebody asks you what the chances are of getting heads each time in ten tossings you can easily give the answer by multiplying $\frac{1}{2}$ by $\frac{1}{2}$ ten times.
The result will be 0.00098, indicating that the chances are very low indeed: about one chance out of a thousand!
Here we have the rule of &ldquo;multiplication of probabilities,&rdquo; which states that if you want several different things, you may determine the mathematical probability of getting them by multiplying the mathematical probabilities of getting the several individual ones.
If there are many things you want, and each of them is not particularly probable, the chances that you get them all are discouragingly low!

There is also another rule, that of the &ldquo;addition of probabilities,&rdquo; which states that if you want only one of several things (no matter which one), the mathematical probability of getting it is the sum of mathematical probabilities of getting individual items on your list.

This can be easily illustrated in the example of getting an equal division between heads and tails in tossing a coin twice.
What you actually want here is either &ldquo;heads once, tails twice&rdquo; or &ldquo;tails twice, heads once.&rdquo;
The probability of each of the above combinations is $\frac{1}{4}$, and the probability of getting either one of them is $\frac{1}{4}$ plus $\frac{1}{4}$ or $\frac{1}{2}$.
Thus: If you want &ldquo;that, and that, and that &hellip; &rdquo; you <em>multiply</em> the individual mathematical probabilities of different items.
If, however, you want &ldquo;that, or that, or that&rdquo; you add the probabilities.


In the first case your chances of getting everything you ask for will decrease as the number of desired items increases. In the second case, when you want only one out of several items your chances of being satisfied increase as the list of items from which to choose becomes longer.

The experiments with tossing coins furnish a fine example of what is meant by saying that the laws of probability become more exact when you deal with a large number of trials.
This is illustrated in Figure \ref{fig:84}, which represents the probabilities of getting a different relative number of heads and tails for two,  three, four, ten, and a hundred tossings.
You see that with the increasing number of tossings the probability curve becomes sharper and sharper and the maximum at fifty-fifty ratio of heads and tails becomes more and more pronounced.


Thus whereas for 2 or 3, or even 4 tosses, the chances to have heads each time or tails each time are still quite appreciable, in 10 tosses even 90 per cent of heads or tails is very improbable.

<figure>
\label{fig:84}
\caption{Relative number of tails and heads}
</figure>

For a still larger number of tosses, say 100 or 1000, the probability curve becomes as sharp as a needle, and the chances of getting even a small deviation from fifty-fifty distribution becomes practically nil.


Let us now use the simple rules of probability calculus that we have just learned in order to judge the relative probabilities of various combinations of five playing cards which one encounters in the well-known game of poker.


In case you do not know, each player in this game is dealt 5 cards and the one who gets the highest combination takes the bank.
We shall omit here the additional complications arising from the possibility of exchanging some of your cards with the hope of getting better ones, and the psychological strategy of bluffing your opponents into submission by making them believe that you have much better cards than you actually have.
Although this bluffing actually is the heart of the game, and once led the famous Danish physicist Niels Bohr to propose an entirely new type of game in which no cards are used, and the players simply bluff one another by talking about the imaginary combinations they have, it lies entirely outside the domain of probability calculus, being a purely psychological matter.

<figure>
    \label{fig:85}
    \caption{A flush (of spades).}
</figure>

In order to get some exercise in probability calculus, let us calculate the probabilities of some of the combinations in the game of poker.
One of these combinations is called a &ldquo;flush&rdquo; and
represents 5 cards all of the same suit (Figure 85).

If you want to get a flush it is immaterial what the first card you get is, and one has only to calculate the chances that the other four will be of the same suit.
There are altogether 52 cards in the pack, 13 cards of each suit, [^4] so that after you get your first card, there remain in the pack 12 cards of the same suit.
Thus the chances that your second card will be of the proper suit are $\frac{12}{51}$.
Similarly the chances that the third, fourth, and fifth cards will be of the same suit are given by the fractions: $\frac{11}{50}$, $\frac{10}{49}$ and $\frac{9}{48}$.
Since you want all 5 cards to be of the same suit you have to apply the rule of probability-multiplications.
Doing this you find that the probability of getting a flush is:

\begin{equation}
    \frac{12}{51} \times \frac{11}{50} \times \frac{10}{49} \times \frac{9}{48} = \frac{13068}{5997600} \text{or about 1 in 500}
\end{equation}

But please do not think that in 500 hands you are sure to get a flush.
You may get none, or you may get two.
This is only probability calculus, and it may happen that you will be dealt many more than 500 hands without getting the desired combination, or on the contrary that you may be dealt a flush the very first time you have the cards in your hands.

<figure>
    \label{fig:86}
    \caption{Full house.}
</figure>

All that the theory of probability can tell you is that you will probably be dealt 1 flush in 500 hands.
You may also learn, by following the same methods of calculation, that in playing 30,000,000 games you will probably get 5 aces (including the joker) about ten times.


Another combination in poker, which is even rarer and therefore more valuable, is the so-called &ldquo;full hand,&rdquo; more popularly called &ldquo;full house.&rdquo;
A full house consists of a &ldquo;pair&rdquo; and &ldquo;three of a kind&rdquo; (that is, 2 cards of the same value in 2 suits, and 3 cards of the same value in 3 suits&mdash;as, for example, the 2 fives and 3 queens shown in Figure 86).

If you want to get a full house, it is immaterial which 2 cards you get first, but when you get them you must have 2 of the remaining 3 cards match one of them, and the other match the other one.
Since there are 6 cards that will match the ones you have (if you have a queen and a five, there are 3 other queens and 3 other fives) the chances that the third card is a right one are 6 out of 50 or $\frac{6}{50}$.
The chances that the fourth card will be the right one are $\frac{5}{49}$ since there are now only 5 right cards out of 49 cards left, and the chance that the fifth card will be right is $\frac{4}{48}$.
Thus the total probability of a full house is:

\begin{equation}
    \frac{6}{50} \times \frac{6}{49} \times \frac{4}{48} = \frac{120}{117600}
\end{equation}

or about one half of the probability of the flush.

In a similar way one can calculate the probabilities of other combinations as, for example, a &ldquo;straight&rdquo; (a sequence of cards), and also take into account the changes in probability introduced by the presence of the joker and the possibility of exchanging the originally dealt cards.

By such calculations one finds that the sequence of seniority used in poker does really correspond to the order of mathematical probabilities.
It is not known by the author whether such an arrangement was proposed by some mathematician of the old times, or was established purely empirically by millions of players risking their money in fashionable gambling salons and little dark haunts all over the world.
If the latter was the case, we must admit that we have here a pretty good statistical study of the relative probabilities of complicated events!

Another interesting example of probability calculation, an example that leads to a quite unexpected answer, is the problem of &ldquo;Coinciding Birthdays.&rdquo;
Try to remember whether you have ever been invited to two different birthday parties on the same day.
You will probably say that the chances of such double invitations are very small since you have only about 24 friends who are likely to invite you, and there are 365 days in the year on which their birthdays may fall.
Thus, with so many possible dates to choose from, there must be very little chance that any 2 of your 24 friends will have to cut their birthday cakes on the same day.


However, unbelievable as it may sound, your judgment here is quite wrong.
The truth is that there is a rather high probability that in a company of 24 people there are a pair, or even several pairs, with coinciding birthdays.
As a matter of fact, there are more chances that there is such a coincidence than that there is not.


You can verify that fact by making a birthday list including about 24 persons, or more simply, by comparing the birth dates of 24 persons whose names appear consecutively on any pages of some such reference book as &ldquo;Who's Who in America,&rdquo; opened at random.
Or the probabilities can be ascertained by using the simple rules of probability calculus with which we have become acquainted in the problems of coin tossing and poker.


Suppose we try first to calculate the chances that in a company of twenty-four persons everyone has a different birth date.
Let us ask the first person in the group what is his birth date; of course this can be any of the 365 days of the year.
Now, what is the chance that the birth date of the second person we approach is different from that of the first?
Since this (second) person could have been born on any day of the year, there is one chance out of 365 that his birth date coincides with that of the first one, and 364 chances out of 365 (i.e., the probability of $\frac{364}{365}$) that it does not.
Similarly, the probability that the third person has a birth date different from that of either the first or second is $\frac{363}{365}$, since two days of the year have been excluded.
The probabilities that the next persons we ask have different birth dates from the ones we have approached before are then: $\frac{362}{365}$, $\frac{361}{365}$, $\frac{360}{365}$ and so on up to the last person for whom the probability is $\frac{(365-23)}{365}$ or $\frac{342}{365}$.


Since we are trying to learn what the probability is that one of these coincidences of birth dates exists, we have to multiply all the above fractions, thus obtaining for the probability of all the persons having different birth dates the value:

\begin{equation}
    \frac{364}{365} \times \frac{363}{365} \times \frac{362}{365} \times \cdots \frac{342}{365}
\end{equation}

One can arrive at the product in a few minutes by using certain methods of higher mathematics, but if you don't know them you can do it the hard way by direct multiplication, [^5] which would not take so very much time.
The result is 0.46, indicating that the probability that there will be no coinciding birthdays is slightly less than one half.
In other words there are only 46 chances in 100 that no two of your two dozen friends will have birthdays on the same day, and 54 chances in 100 that two or more will.
Thus if you have 25 or more friends, and have never been invited to two birthday parties on the same date you may conclude with a high degree of probability that either most of your friends do not organize their birthday parties, or that they do not invite you to them!


The problem of coincident birthdays represents a very fine example of how a common-sense judgment concerning the probabilities of complex events can be entirely wrong.
The author has put this question to a great many people, including many prominent scientists, and in all cases except one [^6] was offered bets ranging from 2 to 1 to 15 to 1 that no such coincidence will occur.
If he had accepted all these bets he would be a rich man by now!

It cannot be repeated too often that if we calculate the probabilities of different events according to the given rules and pick out the most probable of them, we are not at all sure that this is exactly what is going to happen.
Unless the number of tests we are making runs into thousands, millions or still better into billions, the predicted results are only &ldquo;likely&rdquo; and not at all &ldquo;certain.&rdquo;
This slackening of the laws of probability when dealing with a comparatively small number of tests limits, for example, the usefulness of statistical analysis for deciphering various codes and cryptograms which are limited only to comparatively short notes.
Let us examine, for example, the famous case described by Edgar Allan Poe in his well-known story &ldquo;The Gold Bug.&rdquo; He tells us about a certain Mr. Legrand who, strolling along a deserted beach in South Carolina, picked up a piece of parchment half buried in the wet sand.
When subjected to the warmth of the fire burning gaily in Mr. Legrand's beach hut, the parchment revealed some mysterious signs written in ink which was invisible when cold, but which turned red and was quite legible when heated.
There was a picture of a skull, suggesting that the document was written by a pirate, the head of a goat, proving beyond any doubt that the pirate was none other than the famous Captain Kidd, and several lines of typographical signs apparently indicating the whereabouts of a hidden treasure (see Figure 87).

We take it on the authority of Edgar Allan Poe that the pirates of the seventeenth century were acquainted with such typographical signs as semicolons and quotation marks, and such others as:
%% TOTO Special symbols
    $|$, $+$, and <span class="math">j</span>.

Being in need of money, Mr. Legrand used all his mental powers in an attempt to decipher the mysterious cryptogram and

<figure id="fig87">
<figcaption>
Captain Kidd's Message.
</figcaption>
</figure>

finally did so on the basis of the relative frequency of occurrence of different letters in the English language.
His method was based on the fact that if you count the number of different letters of any English text, whether in a Shakespearian sonnet or an Edgar Wallace mystery story, you will find that the letter &ldquo;e&rdquo; occurs by far most frequently.
After &ldquo;e&rdquo; the succession of most frequent letters is as follows:

<math xmlns="http://www.w3.org/1998/Math/MathML" alttext="a, o, i, d, h, n, r, s, t, u, y, c, f, g, l, m, w, b, k, p, q, x, z" display="block", id="eq8-1">
  <mrow>
    <mi>a</mi><mo>, </mo><mi>o</mi><mo>, </mo><mi>i</mi><mo>, </mo><mi>d</mi><mo>, </mo><mi>h</mi><mo>, </mo><mi>n</mi><mo>, </mo><mi>r</mi><mo>, </mo><mi>r</mi><mo>, </mo><mi>s</mi><mo>, </mo><mi>t</mi><mo>, </mo><mi>u</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>c</mi><mo>, </mo><mi>f</mi><mo>, </mo><mi>g</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>m</mi><mo>, </mo><mi>w</mi><mo>, </mo><mi>b</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>p</mi><mo>, </mo><mi>q</mi><mo>, </mo><mi>x</mi><mo>, </mo><mi>z</mi><mo>, </mo>
  </mrow>
</math>

By counting the different symbols appearing in Captain Kidd's cryptogram, Mr. Legrand found that the symbol that occurred most frequent in the message was the figure 8.
&ldquo;Aha,&rdquo; he said, &ldquo;that means that 8 most probably stands for the letter e.&rdquo;


Well, he was right in this case, but of course it was only very probable and not at all certain. In fact if the secret message had been &ldquo;You will find a lot of gold and coins in an iron box in woods two thousand yards south from an old hut on Bird Island's north tip&rdquo; it would not have contained a single &ldquo;e&rdquo;! But the laws of chance were favorable to Mr. Legrand, and his guess was really correct.


Having met with success in the first step, Mr. Legrand became overconfident and proceeded in the same way by picking up the letters in the order of the probability of their occurrence.
In the following table we give the symbols appearing in Captain Kidd's message in the order of their relative frequency of use:


\begin{table}
%\begin{tabu}
%Of the character 8 there are 33
%\end{tabu}
\end{table}


The first column on the right contains the letters of the alphabet arranged in the order of their relative frequency in the English language.
Therefore it was logical to assume that the signs hosted in the broad column to the left stood for the letters listed opposite them in the first narrow column to the right.
But using this arrangement we find that the beginning of Captain Kidd's message reads: <em>ngiisgunddrhaoecr</em> &hellip;

No sense at all!

What happened?
Was the old pirate so tricky as to use special words that do not contain letters that follow the same rules of frequency as those in the words normally used in the English language?
Not at all; it is simply that the text of the message is not long enough for good statistical sampling and the most probable distribution of letters does not occur.
Had Captain Kidd hidden his treasure in such an elaborate way that the instructions for its recovery occupied a couple of pages, or, still better an entire volume, Mr. Legrand would have had a much better chance to solve the riddle by applying the rules of frequency.

If you drop a coin 100 times you may be pretty sure that it will fall with the head up about 50 times, but in only 4 drops you may have heads three times and tails once or vice versa.
To make a rule of it, the larger the number of trials, the more accurately the laws of probability operate.


Since the simple method of statistical analysis failed because of an insufficient number of letters in the cryptogram, Mr. Legrand had to use an analysis based on the detailed structure of different words in the English language.
First of all he strengthened his hypothesis that the most frequent sign 8 stood for e by noticing that the combination 88 occurred very often (5 times) in this comparatively short message, for, as everyone knows, the letter <span class="math">e</span> is very often doubled in English words (as in: meet, fleet, speed, seen, been, agree, etc.).
Furthermore if 8 really stood for <span class="math">e</span> one would expect it to occur very often as a part of the word &ldquo;the.&rdquo; Inspecting the text of the cryptogram we find that the combination ;48 occurs seven times in a few short lines.
But if this is true, we must conclude that ; stands for <span class="math">t</span> and 4 for <span class="math">h</span>.

We refer the reader to the original Poe story for the details concerning the further steps in the deciphering of Captain Kidd's message, the complete text of which was finally found to be:
&ldquo;A good glass in the bishop's hostel in the devil's seat.
Forty-one degrees and thirteen minutes northeast by north.
Main branch seventh limb east side. Shoot from the left eye of the death's head.
A bee-line from the tree through the shot fifty feet out.&rdquo;

The correct meaning of the different characters as finally deciphered by Mr. Legrand is shown in the second column of the table on page 217, and you see that they do not correspond exactly to the distribution that might reasonably be expected on the basis of the laws of probability.
It is, of course, because the text is too short and therefore does not furnish an ample opportunity for the laws of probability to operate.
But even in this small &ldquo;statistical sample&rdquo; we can notice the tendency for the letters to arrange themselves in the order required by the theory of probability, a tendency that would become almost an unbreakable rule if the number of letters in the message were much larger.

<figure>
    \label{fig:88}
</figure>

There seems to be only one example (excepting the fact that insurance companies do not break up) in which the predictions of the theory of probability have actually been checked by a very large number of trials.
This is a famous problem of the American flag and a box of kitchen matches.


To tackle this particular problem of probability you will need an American flag, that is, the part of it consisting of red and white stripes; if no flag is available just take a large piece of paper and draw on it a number of parallel and equidistant lines.
Then you need a box of matches&mdash;any kind of matches, provided they are shorter than the width of the stripes.
Next you will need a Greek pi, which is not something to eat, but just a letter of the Greek alphabet equivalent to our &ldquo;p.&rdquo;
It looks like this: $\pi$.
In addition to being a letter of the Greek alphabet, it is used to signify the ratio of the circumference of a circle to its diameter.
You may know that numerically it equals 3.1415926535&hellip; (many more digits are known, but we shall not need them all.)


Now spread the flag on a table, toss a match in the air and watch it fall on the flag (Figure \ref{fig:88}.
It may fall in such a way that it all remains within one stripe, or it may fall across the boundary between two stripes.
What are the chances that one or another will take place?


Following our procedure in ascertaining other probabilities.
we must first count the number of cases that correspond to one or another possibility.


But how can you count all the possibilities when it is clear that a match can fall on a flag in an infinite number of different ways?


Let us examine the question a little more closely.
The position of the fallen match in respect to the stripe on which it falls can be characterized by the distance of the middle of the match from the nearest boundary line, and by the angle that the match forms with the direction of the stripes in Figure \ref{fig:89}.
We give three typical examples of fallen matches, assuming, for the sake of simplicity, that the length of the match equals the width of the stripe, each being, say, two inches.
If the center of the match is rather close to the boundary line, and the angle is rather large (as in case a) the match will intersect the line.
If, on the contrary, the angle is small (as in case b) or the distance is large (as in case c) the match will remain within the boundaries of one stripe.
More exactly we may say that the match will intersect the line if the projection of the half-of-the-match on the vertical direction is larger than the half width of the stripe (as in case a), and that no intersection will take place if the opposite is true (as in case b).
The above statement is represented graphically on the diagram in the lower part of the picture.
We plot on the horizontal axis ( abscissa ) the angle of the fallen match as given by the length of the corresponding arc of radius 1.
On the vertical axis (ordinate) we plot the length of the projection of the half-match length on the vertical direction; in trigonometry this length is known as the sinus corresponding to the given arc.
It is clear that the sinus is zero when the arc is zero since in that case the match occupies a horizontal position.
When the arc is $\frac{1}{2}\pi$, which corresponds to a straight angle, [^7] the sinus is equal to unity, since the match occupies a vertical position and thus coincides with its projection.
For intermediate values of the arc the sinus is given by the familiar mathematical wavy curve known as sinusoid. (In Figure \ref{fig:89} we have only one quarter of a complete wave in the interval between <span class="math">0</span> and $\frac{\pi}{2}$.)

<figure>
    \label{fig:89}
</figure>


Having constructed this diagram we can use it with convenience for estimating the chances that the fallen match will or will not cross the line.
In fact, as we have seen above (look again at the three examples in the upper part of Figure \ref{fig:89}) the match will cross the boundary line of a stripe if the distance of the center of the match from the boundary line is less than the corresponding projection, that is, less than the sinus of the arc.
That means that in plotting that distance and that arc in our diagram we get a point below the sinus line.
On the contrary the match that falls entirely within the boundaries of a stripe will give a point above the sinus line.


Thus, according to our rules for calculating probabilities, the chances of intersection will stand in the same ratio to the chances of nonintersection as the area below the curve does to the area above it; or the probabilities of the two events may be calculated by dividing the two areas by the entire area of the rectangle.
It can be proved mathematically (<em>cf</em>. Chapter II) that the area of the sinusoid presented in our diagram equals exactly

1. Since the total area of the rectangle is $\frac{\pi}{2} \times 1 = \frac{\pi}{2}$ we find the probability that the match will fall across the boundary (for matches equal in length to the stripe width) is: $\frac{1}{\frac{\pi}{2}}=\frac{2}{\pi}$.


The interesting fact that $\pi$ pops up here where it might be least expected was first observed by the eighteenth century scientist Count Buffon, and so the match-and-stripes problem now bears his name.


An actual experiment was carried out by a diligent Italian mathematician, Lazzerini, who made 3408 match tosses and observed that 2169 of them intersected the boundary line.
The exact record of this experiment, checked with the Buffon formula, substitutes for tt a value of $\frac{2+3408}{2169}$ or 3.1415929, differing from the exact mathematical value only in the seventh decimal place! This represents, of course, a most amusing proof of the validity of the probability laws, but not more amusing than the determination of a number &ldquo;2&rdquo; by tossing a coin several thousand times and dividing the total number of tosses by the number of times heads come up.
Sure enough you get in this case: 2.000000&hellip; with just as small an error as in Lazzerini's determination of $\pi$.


## The &ldquo;Mysterious&rdquo; Entropy


From the above examples of probability calculus, all of them pertaining to ordinary life, we have learned that predictions of that sort, being often disappointing when small numbers are involved, become better and better when we go to really large numbers.
This makes these laws particularly applicable to the description of the almost innumerable quantities of atoms or molecules that form even the smallest piece of matter we can conveniently handle.
Thus, whereas the statistical law of Drunkard's Walk can give us only approximate results when applied to a half-dozen drunkards who make perhaps two dozen turns each, its application to billions of dye molecules undergoing billions of collisions every second leads to the most rigorous physical law of diffusion.
We can also say that the dye that was originally dissolved in only one half of the water in the test tube tends through the process of diffusion to spread uniformly through the entire liquid, because, such uniform distribution is more probable than the original one.



For exactly the same reason the room in which you sit reading this book is filled uniformly by air from wall to wall and from floor to ceiling, and it never even occurs to you that the air in the room can unexpectedly collect itself in a far corner, leaving you to suffocate in your chair.
However, this horrifying event is not at all physically impossible, but only highly improbable.


To clarify the situation, let us consider a room divided into two equal halves by an imaginary vertical plane, and ask ourselves about the most probable distribution of air molecules between the two parts.
The problem is of course identical with the coin-tossing problem discussed in the previous chapter.
If we pick up one single molecule it has equal chances of being in the right or in the left half of the room, in exactly the same way as the tossed coin can fall on the table with heads or tails up.


The second, the third, and all the other molecules also have equal chances of being in the right or in the left part of the room regardless of where the others are. [^8]
Thus the problem of distributing molecules between the two halves of the room is equivalent to the problem of heads-and-tails distribution in a large number of tosses, and as you have seen from Figure 84, the fifty-fifty distribution is in this case by far the most probable one.
We also see from that figure that with the increasing number of tosses (the number of air molecules in our case) the probability at 50 per cent becomes greater and greater, turning practically into a certainty when this number becomes very large.
Since in the average-size room there are about $10^{27}$ molecules, [^9]
the probability that all of them collect simultaneously in, let us say, the right part of the room is:

<math xmlns="http://www.w3.org/1998/Math/MathML" alttext="\left(\frac{1}{2}\right)^{10^{27}} \approx 10^{3\times 10^{26}}" display="block", id="eq8-1">
  <mrow>
    <msup>
      <mfenced>
        <mfrac>
          <mn>1</mn>
          <mn>2</mn>
        </mfrac>
      </mfenced>
      <msup>
        <mn>10</mn>
        <mn>27</mn>
      </msup>
    </msup>
    <mo>&sim;</mo>
    <mrow>
      <msup>
        <mn>10</mn>
        <mrow>
          <mn>3</mn>
          <mo>&times;</mn>
          <msup>
            <mn>10</mn>
            <mn>26</mn>
          </msup>
        </mrow>
      </msup>
    </mrow>
  </mrow>
</math>

i.e., 1 out of $10^{3\times 10^{10}}$

On the other hand, since the molecules of air moving at the speed of about 0.5 km/sec require only 0.01 second to move from one end of the room to the other, their distribution in the room will be reshuffled 100 times each second.
Consequently the waiting time for the right combination is
$10^{299,999,999,909,999,999,999,999,998\,<span class="textsc">second</span>}$ as compared with only $10^{17}$ second representing the total age of the universe!
Thus you may go on quietly reading your book without being afraid of being suffocated by chance.


To take another example, let us consider a glass of water standing on the table.
We know that the molecules of water, being involved in the irregular thermal motion, are moving at high speed in all possible directions, being, however, prevented from flying apart by the cohesive forces between them.

Since the direction of motion of each separate molecule is governed entirely by the law of chance, we may consider the possibility that at a certain moment the velocities of one half of the molecules, namely those in the upper part of the glass, will all be directed upward, whereas the other half, in the lower part of the glass, will move downwards. [^10]
In such a case, the cohesive forces acting along the horizontal plane dividing two groups of molecules will not be able to oppose their &ldquo;unified desire for parting,&rdquo; and we shall observe the unusual physical phenomenon of half the water from the glass being spontaneously shot up with the speed of a bullet toward the ceiling!


Another possibility is that the total energy of thermal motion of water molecules will be concentrated by chance in those located in the upper part of the glass, in which case the water near the bottom suddenly freezes, whereas its upper layers begin to boil violently.
Why have you never seen such things happen?
Not because they are absolutely impossible, but only because they are extremely improbable.
In fact, if you try to calculate the probability that molecular velocities, originally distributed at random in all directions, will by pure chance assume the distribution described above, you arrive at a figure that is just about as small as the probability that the molecules of air will collect in one comer.
In a similar way, the chance that, because of mutual collisions, some of the molecules will lose most of their kinetic energy, while the other part gets a considerable excess of it, is also negligibly small.
Here again the distribution of velocities that corresponds to the usually observed case is the one that possesses the largest probability.


If now we start with a case that does not correspond to the most probable arrangement of molecular positions or velocities, by letting out some gas in one comer of the room, or by pouring some hot water on top of the cold, a sequence of physical changes will take place that will bring our system from this less probable to a most probable state.
The gas will diffuse through the room until it fills it up uniformly, and the heat from the top of the glass will flow toward the bottom until all the water assumes an equal temperature.
<em>Thus we may say that all physical processes depending on the irregular motion of molecules go in the direction of increasing probability, and the state of equilibrium, when nothing more happens, corresponds to the maximum of probability.</em>
Since, as we have seen from the example of the air in the room, the probabilities of various molecular distributions are often expressed by inconveniently small numbers (as $10^{-3\cdot 10^{28}}$ for the air collecting in one half of the room), it is customary to refer to their logarithms instead.
This quantity is known by the name of entropy, and plays a prominent role in all questions connected with the irregular thermal motion of matter.
The foregoing statement concerning the probability changes in physical processes can be now rewritten in the form:
    <em>Any spontaneous changes in a physical system occur in the direction of increasing entropy, and the first state of equilibrium corresponds to the maximum possible value of the entropy.</em>

This is the famous <em>Law of Entropy</em>, also known as the Second Law of Thermodynamics (the First Law being the Law of Conservation of Energy), and as you see there is nothing in it to frighten you.

The Law of Entropy can also be called the <em>Law of Increasing Disorder</em> since, as we have seen in all the examples given above, the entropy reaches its maximum when the position and velocities of molecules are distributed completely at random so that any attempt to introduce some order in their motion would lead to the decrease of the entropy.
Still another, more practical, formulation of the Law of Entropy can be obtained by reference to the problem of turning the heat into mechanical motion.
Remembering that the heat is actually the disorderly mechanical motion of molecules, it is easy to understand that the complete transformation of the heat content of a given material body into mechanical energy of large-scale motion is equivalent to the task of forcing all molecules of that body to move in the same direction.
However, in the example of the glass of water that might spontaneously shoot one half of its contents toward the ceiling, we have seen that such a phenomenon is sufficiently improbable to be considered as being practically impossible.
Thus, <em>although the energy of mechanical motion can go completely over into heat (for example, through friction), the heat energy can never go completely into mechanical motion.</em>
This rules out the possibility of the so-called &ldquo;perpetual motion motor of the second kind, [^11]
which would extract the heat from the material bodies at normal temperature, thus cooling them down and utilizing for doing mechanical work the energy so obtained.
For example, it is impossible to build a steamship in the boiler of which steam is generated not by burning coal but by extracting the heat from the ocean water, which is first pumped into the engine room, and then thrown back overboard in the form of ice cubes after the heat is extracted from it.


But how then do the ordinary steam-engines turn the heat into motion without violating the Law of Entropy?
The trick is made possible by the fact that in the steam engine only a part of the heat liberated by burning fuel is actually turned into energy, another larger part being thrown out into the air in the form of exhaust steam, or absorbed by the specially arranged steam coolers.
In this case we have two opposite changes of entropy in our system: (1) the increase of entropy corresponding to the transformation of a part of the heat into mechanical energy of the pistons, and (2) the decrease of entropy resulting from the flow of another part of the heat from the hot-water boilers into the coolers.
The Law of Entropy requires only that the total amount of entropy of the system increase, and this can be easily arranged by making the second factor larger than the first.
The situation can probably be understood somewhat better by considering an example of a 5 lb weight placed on a shelf 6 feet above the floor. According to the Law of Conservation of Energy, it is quite impossible that this weight will spontaneously and without any external help rise toward the ceiling.
On the other hand it is possible to drop one part of this weight to the floor and use the energy thus released to raise another part upward.


In a similar way we can decrease the entropy in one part of our system if there is a compensating increase of entropy in its other part.
In other words considering a disorderly motion of molecules we can bring some order in one region, if we do not mind the fact that this will make the motion in other parts still more disorderly.
And in many practical cases, as in all kinds of heat engines, we do not mind it.

## Statistical Fluctuation

The discussion of the previous section must have made it clear to you that the Law of Entropy and all its consequences is based entirely on the fact that in large-scale physics we are always dealing with an immensely large number of separate molecules, so that any prediction based on probability considerations becomes almost an absolute certainty.
However, this kind of prediction becomes considerably less certain when we consider very small amounts of matter.


Thus, for example, if instead of considering the air filling a large room, as in the previous example, we take a much smaller volume of gas, say a cube measuring one hundredth of a micron
\footnote{
    One micron, usually denoted by Greek letter Mu ($\mu$), is 0.0001 centimeter.
}
each way, the situation will look entirely different.
In fact, since the volume of our cube is $10^{-13}$ centimeter cubed it will contain only $\frac{10^{-18} 10^{-3}}{3\times 10^{-23}}$ 30 molecules, and the chance that all of them will collect in one half of the original volume is $\left(\frac{1}{2}\right)^30 = 10^{-10}$.

On the other hand, because of the much smaller size of the cube, the molecules will be reshuffled at the rate of $5\times 10^9$ times per second (velocity of 0.5 km/sec) and the distance of only $10^{-6}$ centimeter) so that about once every second we shall find that one half of the cube is empty.
It goes without saying that the cases when only a certain fraction of molecules become concentrated at one end of our small cube occur considerably more often.
Thus for example the distribution in which 20 molecules are at one end and 10 molecules at the other (i.e only 10 extra molecules collected at one end) will occur with the frequency of $\left(\frac{1}{2}\right)^{10} \times 5\times 10^{10} = 10^{-3} \times 5 \times 10^{10} = 5\times 10^{7}$, that is, 50,000,000 times per second.


Thus, on a small scale, the distribution of molecules in the air is far from being uniform.
If we could use sufficient magnification, we should notice the small concentration of molecules being instantaneously formed at various points of the gas, only to be dissolved again, and be replaced by other similar concentrations appearing at other points.
This effect is known as fluctuation of density and plays an important role in many physical phenomena.
Thus, for example, when the rays of the sun pass through the atmosphere these inhomogeneities cause the scattering of blue rays of the spectrum, giving to the sky its familiar color and making the sun look redder than it actually is.
This effect of reddening is especially pronounced during the sunset, when the sun rays must pass through the thicker layer of air.
Were these fluctuations of density not present the sky would always look completely black and the stars could be seen during the day.


Similar, though less pronounced, fluctuations of density and pressure also take place in ordinary liquids, and another way of describing the cause of Brownian motion is by saying that the tiny particles suspended in the water are pushed to and fro because of rapidly varying changes of pressure acting on their opposite sides.
When the liquid is heated until it is close to its boiling point, the fluctuations of density become more pronounced and cause a slight opalescence.

We can ask ourselves now whether the Law of Entropy applies to such small objects as those to which the statistical fluctuations become of primary importance. 
Certainly a bacterium, which through all its life is tossed around by molecular impacts, will sneer at the statement that heat cannot go over into mechanical motion!
But it would be more correct to say in this case that the Law of Entropy loses its sense, rather than to say that it is violated.
In fact all that this law says is that molecular motion cannot be transformed completely into the motion of large objects containing immense numbers of separate molecules.
For a bacterium, which is not much larger than the molecules themselves, the difference between the thermal and mechanical motion has practically disappeared, and it would consider the molecular collisions tossing it around in the same way as we would consider the kicks we get from our fellow citizens in an excited crowd.
If we were bacteria, we should be able to build a perpetual motion motor of the second kind by simply tying ourselves to a flying wheel, but then we should not have the brains to use it to our advantage.
Thus there is actually no reason for being sorry that we are not bacteria!


[^1]: All values given for atmospheric pressure.

[^2]: See Chapter XI.

[^3]: When we bring a metal wire to a high temperature, the thermal motion of electrons in its inside becomes more violent and some of them come out through the surface. This is the phenomenon used in electron tubes and familiar to all radio amateurs.

[^4]: We omit here the complications arising from the presence of the &ldquo;joker,&rdquo; an extra card which can be substituted for any other card according to the desire of the player.

[^5]: Use a logarithmic table or slide rule if you can!

[^6]: This exception was, of course, a Hungarian mathematician (see the beginning of the first chapter of this book).

[^7]: The circumference of a circle with the radius 1 is <span class="math">&pi;</span>; times its diameter or <span class="math">2&pi;</span>. Thus the length of one quadrant of a circle is <span class="math">2&pi;/4</span> or <span class="math">&pi;/2</span>.

[^8]: In fact, owing to large distances between separate molecules of the gas, the space is not at all crowded and the presence of a large number of molecules in a given volume does not at ail prevent the entrance of new molecules.

[^9]: A room 10 feet by 15 feet, with a 9 feet ceiling has a volume of 1350 feet cubed, or <span class="math">5&times; 10<sup>7</sup></span> centimeter cubed, thus containing <span class="math">5 &times; 10<sup>&minus;23</sup></span> gram of air. Since the average mass of air molecules is <span class="math">3 &times; 1.66 &times; 10<sup>&minus;24</sup> gram = 5 &times; 10<sup>&minus;23</sup> gram</span>, the total number of molecules is <span class="math">5&times;10<sup>4<sup>/3&times;10<sup>&minus;25</sup> = 10<sup>27</sup></span>. (<span class="math">&sim;</span> means: approximately equal to.)

[^10]: We must consider this half-and-half distribution, since the possibility that all molecules move in the same direction is ruled out by the mechanical law of the conservation of momentum.

[^11]: Called so in contrast to the &ldquo;perpetual motion motor of the first kind&rdquo; which violates the law of conservation of energy working without any energy supply.



